{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "reflected-albania",
   "metadata": {},
   "source": [
    "# Multivariate Decomposition\n",
    "This notebook explores various methods to summarize high dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-isaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the pandas database\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import glob\n",
    "import json\n",
    "\n",
    "# to load image data\n",
    "from PIL import Image\n",
    "\n",
    "# set nice defaults for seaborn plotting library\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-jersey",
   "metadata": {},
   "source": [
    "As we have seen previously, adding a modest number of experimental variables can dramatically increase the complexity of the analysis. This problem is generally referred to as [the curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality). To combat the issues associated with high-dimensional data, [dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction) methods have been developed. These methods are closely related to the concept of data compression. In either case the end result is a transformation that reduces the size of the original data, while preserving the important information.   \n",
    "\n",
    "We will explore this problem with the faces dataset. Let's start with creating the familiar `facedb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-bonus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_facedb(dir=\"/srv/data/faces/generated.photos\"):\n",
    "    '''create a database of all the generated faces found in the supplied directory'''\n",
    "    db = pd.DataFrame()\n",
    "    pattern = dir + \"/*.jpg\"\n",
    "    face_photos = glob.glob(pattern)\n",
    "    db[\"fullpath\"] = face_photos\n",
    "    partitioned = db[\"fullpath\"].str.rpartition(\"/\")\n",
    "    db[\"filename\"] = partitioned.values[:, 2]\n",
    "    db[\"faceid\"] = db[\"filename\"].str.extract(r\"_(\\d{7})[_.]\")\n",
    "\n",
    "    json_files = db[\"fullpath\"].str.replace(\"/generated.photos/\", \n",
    "                                            \"/generated.photos_metadata/\", regex=False)\n",
    "    json_files = json_files.str.replace(\".jpg\", \".json\", regex=False)\n",
    "    db[\"jsonfile\"] = json_files\n",
    "    return db\n",
    "\n",
    "def read_metadata(facedb, id):\n",
    "    '''return a string containing the \\\"faceAttributes\\\" for the given face'''\n",
    "    with open(facedb[\"jsonfile\"][id]) as f:\n",
    "        data = json.load(f)\n",
    "    attributeInfo = pprint.pformat(data[\"faceAttributes\"])\n",
    "    return attributeInfo\n",
    "\n",
    "# call create_facedb() to create a pandas DataFrame called \"facedb\"\n",
    "facedb = create_facedb()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-professor",
   "metadata": {},
   "source": [
    "RGB images can be stored as a 3D matrix, where each pixel defined by values for red, green, and blue. This can be converted to grayscale using a weighted sum of the R, G, and B instensity values. The end result is a single value for each pixel that describes the \"brightness\" of that pixel. Using this conversion, images can be stored as a 2D numpy array. \n",
    "\n",
    "Now we will create a function that converts multiple images from the face dataset to gray scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-trade",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsample = 5\n",
    "def get_pixel_data(face_ids):\n",
    "    data_list = []\n",
    "    for id in face_ids:\n",
    "        im = Image.open(facedb[\"fullpath\"][id], 'r')\n",
    "        im_data = np.array(im.getdata())\n",
    "        rgb_weights = [0.2989, 0.5870, 0.1140] # Used to convert images to grayscale\n",
    "        gray_image =  np.dot(im_data, rgb_weights).reshape(256,256)\n",
    "        data_list.append(gray_image[::downsample, ::downsample]) # Images are downsampled to improve efficiency\n",
    "    return np.array(data_list)\n",
    "num_images = 100\n",
    "image_list = get_pixel_data(range(num_images))\n",
    "image_shape = image_list.shape[1:] # Grab the dimensions of a single image for reshaping later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-carbon",
   "metadata": {},
   "source": [
    "This function returns a 3D numpy array, where the first dimmension indexes a single picture, and the following 2 dimensions are the height and width. The structure can be imagined as a stack of gray-scale photos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-monthly",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_list.shape)\n",
    "plt.imshow(image_list[9,:], cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-columbus",
   "metadata": {},
   "source": [
    "Previously we discussed the notion of \"face embeddings\". Specifically, assigning a vector of values to each image that somehow capture the relationships between images. For face classification, vectors that are similar to each other are more likely to be the same person. An obvious question to ask is: how do we create these face embeddings?. Now that we have converted a set of images into a 3D numpy array, the question can be rephrased as: how do we make the array smaller, without losing information about the faces?\n",
    "\n",
    "The most common approach to dimensionality reduction is [principal component analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) (PCA). The method takes in a 2D matrix, where each row corresponds to observations (or samples from an experiment), and each column corresponds to a variable. The method provides a simple transformation that reduces the number of variables necessary to describe the data. The \"components\", are essentially functions that take a sample with n_features, and outputs a single value that is unique for each image: `COMPONENT1(feature1, feature1, feature3, ...) -> value`. Adding more components allows for a more accurate reconstruction of the original data. \n",
    "\n",
    "Succinctly, (n_samples, n_features) --PCA--> (n_samples, n_components). \n",
    "\n",
    "To make our image data compatible with PCA, we will have to reshape it into a 2D matrix. For a single image, this is equivalent to arranging every pixel into a single line. The final output will be a new matrix with the shape (n_subjects, n_pixels). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-stick",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_flattened = image_list.reshape(num_images, -1) \n",
    "print(image_list.shape)\n",
    "print(image_flattened.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-wells",
   "metadata": {},
   "source": [
    "Let's see how the reconstruction changes as a function of the number of components. We will utilize the scikit-learn package to perform PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-blank",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_selection = 50 #Choose an image to reconstruct\n",
    "n_components = 10 # Select how many components to keep\n",
    "pca = PCA(n_components=n_components, random_state=0)\n",
    "transformed = pca.fit_transform(image_flattened)\n",
    "reconstruction = pca.inverse_transform(transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-sudan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code demonstrates how the reconstruction quality changes \n",
    "image_selection = 50 #Choose an image to reconstruct\n",
    "\n",
    "fig, axs = plt.subplots(1, 5, figsize=(12,6), tight_layout=True)\n",
    "for idx, ax in zip(range(0,50,10), axs.reshape(-1)):\n",
    "    pca = PCA(n_components=idx + 1, random_state=0)\n",
    "    transformed = pca.fit_transform(image_flattened)\n",
    "    reconstruction = pca.inverse_transform(transformed)\n",
    "    ax.imshow(reconstruction[image_selection,:].reshape(image_shape), cmap='gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.set_title(f'{idx+1} components')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-commercial",
   "metadata": {},
   "source": [
    "As you can see, adding more components results in a clearer image. A nice benefit of PCA is that components are ordered in terms of \"importance\". The first few principal components capture the majority of the information in the dataset, but there tends to be diminishing returns as more components are added.\n",
    "\n",
    "Now you may be wondering what these \"components\" actually represent, and how they are used to reconstruct new images. In the case of images, you can actually think of them as transparent layers that represent different features of the faces. By changing the transparency (or weight) of these layers, you can reconstruct different images.\n",
    "\n",
    "To illustrate this, we will randomly pick some components and plot the features they represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-detroit",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 50\n",
    "component_list = sorted(np.random.choice(range(n_components), size=9))\n",
    "pca = PCA(n_components=n_components, random_state=0)\n",
    "transformed = pca.fit_transform(image_flattened)\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, figsize=(8,8), tight_layout=True)\n",
    "for ax, idx in zip(axs.reshape(-1), component_list):\n",
    "    eigen_face = pca.components_[idx, :].reshape(image_shape)\n",
    "    ax.imshow(eigen_face, cmap='gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.set_title(f'PC{idx+1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-contact",
   "metadata": {},
   "source": [
    "These visualizations are known as [eigen faces](https://en.wikipedia.org/wiki/Eigenface) which refers to the term eigenvectors, an alternative word for principal components that reflects the math behind the algorithm. Since each image is a weighted combination of these eigen faces, we can actually make artifical faces by choosing random weights and adding the components together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 50\n",
    "weights = np.random.rand(50)*200 - 5\n",
    "new_image = np.zeros(image_shape)\n",
    "for idx in range(n_components):\n",
    "    new_image += weights[idx] * pca.components_[idx, :].reshape(image_shape) * pca.explained_variance_[idx] \n",
    "    \n",
    "plt.imshow(new_image, cmap='gray')    \n",
    "plt.title('Artifical Face')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-ticket",
   "metadata": {},
   "source": [
    "A final feature that is quite useful for PCA is data visualization. It is often not feasible to efficiently visualize datasets with >3 features (dimensions) per sample. PCA provides a method to view each sample in a lower dimensional space. Specifically, we can plot each sample according to the weights of the first 2 components. Since these first 2 components capture more information about the data, samples that are \"closer\" in this low-dimensional space tend to be more similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-infrastructure",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "x, y = transformed[:,0], transformed[:,1]\n",
    "plt.scatter(x,y)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('PCA Projection of face dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-qualification",
   "metadata": {},
   "source": [
    "To get a sense for how similar these photos are, we can try clustering the data points. While we can only visualize 3 dimensions, we can leverage all components for clustering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-grocery",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "cluster_predictions = KMeans(n_clusters=5).fit_predict(transformed)\n",
    "plt.scatter(x,y, c=cluster_predictions)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('K-means clustering of face dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-music",
   "metadata": {},
   "source": [
    "Finally, we can pull out images from the clusters and see how they compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-offset",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs_all = plt.subplots(5, 5, figsize=(8,8), tight_layout=True)\n",
    "for cluster_idx, axs in zip(range(5), axs_all.reshape(5, -1)):\n",
    "    cluster_positions = np.where(cluster_predictions == cluster_idx)[0]\n",
    "    image_selection = np.random.choice(cluster_positions, 5)\n",
    "    for image_idx, ax in zip(image_selection, axs):\n",
    "        ax.imshow(image_list[image_idx,:,:], cmap='gray')\n",
    "        ax.set_title(f'Cluster {cluster_idx}')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-dakota",
   "metadata": {},
   "source": [
    "As you can see, clustering using principal components tends to capture head position. However, some clusters do capture the same subject. For face classification, alternative algorithms are necessary to emphasize facial features. Nevertheless, the concepts illustrated in this notebook are shared for the large family of dimensionality reduction methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-today",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
